{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb81f44-dfce-4936-9b83-11088f511c2c",
   "metadata": {},
   "source": [
    "Bonus Attempted\n",
    "1. Automatic learning rate decay\n",
    "2. Changing doc stride - preprocessing\n",
    "3. Automatic mixed precision\n",
    "4. Gradient accumulation\n",
    "5. Post processing\n",
    "6. Other pretraining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c64c54b-d8c4-4a47-b925-f2ff86d24a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import warnings\n",
    "from transformers import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f87ce4-73cc-475c-b62c-f02719aa3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(squad_file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    squad_data = squad_dict['data']\n",
    "    \n",
    "    rows = []\n",
    "    for article in squad_data:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                answer = qa['answers'][0]['text']\n",
    "                answer_start = qa['answers'][0]['answer_start']\n",
    "                answer_end = answer_start + len(answer)\n",
    "                rows.append([str(context), str(question), str(answer), answer_start, answer_end])\n",
    "    return rows\n",
    "\n",
    "\n",
    "def save_to_csv(file_path, rows):\n",
    "    row_count = 0\n",
    "    with open(file_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['context', 'question', 'answer', 'answer_start', 'answer_end'])\n",
    "        for row in rows:\n",
    "            try:\n",
    "                writer.writerow(row)\n",
    "            except:\n",
    "                row_count = row_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34967d51-ce84-41c9-ae36-eaea96172af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train_data_rows = preprocess_squad_data('spoken_train-v1.1.json')\n",
    "save_to_csv('squad_train_data.csv', squad_train_data_rows)\n",
    "\n",
    "squad_test_data_rows = preprocess_squad_data('spoken_test-v1.1.json')\n",
    "save_to_csv('squad_test_data.csv', test_data_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5413c7e0-8749-4b44-ad6c-44898d40f288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0778c946-b662-4608-a7f9-3e49fdd90681",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DataLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc849b6-91e0-431d-9c94-d6cc0dfd2def",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08aece01-4a5c-4fda-bf87-c49b4d9b9d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "class Dataset_Squad(Dataset):\n",
    "    def __init__(self, squad_file_path):\n",
    "        self.contexts = []\n",
    "        self.questions = []\n",
    "        self.answers = []\n",
    "        self.answer_starts = []\n",
    "        self.answer_ends = []\n",
    "\n",
    "        with open(squad_file_path, 'r', encoding='cp1252') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                self.contexts.append(row[0])\n",
    "                self.questions.append(row[1])\n",
    "                self.answers.append(row[2])\n",
    "                self.answer_starts.append(int(row[3]))\n",
    "                self.answer_ends.append(int(row[4]))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'context': self.contexts[idx],\n",
    "            'question': self.questions[idx],\n",
    "            'answer': self.answers[idx],\n",
    "            'answer_start': self.answer_starts[idx],\n",
    "            'answer_end': self.answer_ends[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1269b23b-da52-43a8-850b-403a321f97c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train dataset 37079\n",
      "Length of Test dataset 5351\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset_Squad('squad_train_data.csv')\n",
    "print(f'Length of Train dataset {len(train_dataset)}')\n",
    "\n",
    "test_dataset = Dataset_Squad('squad_test_data.csv')\n",
    "print(f'Length of Test dataset {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e970118-9de2-49f8-910b-4bac7a45903f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train dataloader 4635\n",
      "Length of Train dataloader 669\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(f'Length of Train dataloader {len(train_data_loader)}')\n",
    "\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(f'Length of Train dataloader {len(test_data_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde5d0ac-14de-4b2d-97d4-6d1e12bb3a31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "850d06c1-833c-4c2a-8f42-66b2648aea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def model_train(model, data_loader, optimizer, device, accumulation_steps):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    total_loss = 0\n",
    "    \n",
    "    scaler = GradScaler()  # initialize the GradScaler object\n",
    "    batch_counter = 0\n",
    "\n",
    "    for data in data_loader:\n",
    "        # Move data to device\n",
    "        inputs = tokenizer(\n",
    "            data['context'],\n",
    "            data['question'],\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            stride=128,\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        start_positions = data['answer_start'].to(device)\n",
    "        end_positions = data['answer_end'].to(device)\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():  # enable automatic mixed precision\n",
    "            outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()  # scale the loss and perform backward pass\n",
    "        \n",
    "        batch_counter += 1\n",
    "        if batch_counter % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)  # update the model weights\n",
    "            scaler.update()  # update the GradScaler for the next iteration\n",
    "            optimizer.zero_grad()  # clear gradients\n",
    "            \n",
    "        loss_val = loss.item()\n",
    "        \n",
    "        if str(loss_val) == 'nan':\n",
    "            loss_val = 0\n",
    "\n",
    "        total_loss += loss_val\n",
    "\n",
    "    if batch_counter % accumulation_steps != 0:\n",
    "        scaler.step(optimizer)  # update the model weights\n",
    "        scaler.update()  # update the GradScaler for the next iteration\n",
    "        optimizer.zero_grad()  # clear gradients\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c74dd-7c94-4a0b-bd0a-65f92ede6b69",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a40bcb-80e1-4dc9-885d-95b9e856f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model, data_loader, optimizer, device):    \n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    scaler = GradScaler()  # initialize the GradScaler object\n",
    "    \n",
    "    for data in data_loader:\n",
    "        # Move data to device\n",
    "        inputs = tokenizer(\n",
    "            data['context'],\n",
    "            data['question'],\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            stride=128,\n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        start_positions = data['answer_start'].to(device)\n",
    "        end_positions = data['answer_end'].to(device)\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():  # enable automatic mixed precision\n",
    "            outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        loss_val = loss.item()\n",
    "        \n",
    "        if str(loss_val) == 'nan':\n",
    "            loss_val = 0\n",
    "        \n",
    "        valid_loss += loss_val\n",
    "\n",
    "    return valid_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d8c295-1b06-4ff3-b98b-be11cd1aa67a",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1d13da2-e8d0-4bb3-8f91-a5a07786145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_result(model, tokenizer, context, question):\n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(context, question, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    input_ids = inputs['input_ids'].squeeze()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(**inputs)\n",
    "\n",
    "    # Get predicted answer\n",
    "    start_idx = torch.argmax(output.start_logits)\n",
    "    end_idx = torch.argmax(output.end_logits) + 1\n",
    "    \n",
    "    # what if end index < start index\n",
    "    if end_idx < start_idx:\n",
    "        # Swap the indices if end_idx is less than start_idx\n",
    "        start_idx, end_idx = end_idx, start_idx\n",
    "    \n",
    "    answer = tokenizer.decode(input_ids[start_idx:end_idx])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c111c8-a27c-4c40-b85e-46fee82769d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training & Evaluation - BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "297e2843-5140-415a-81eb-f2a23bb36670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', max_length=512)\n",
    "bert_model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.AdamW(bert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24e85e4-3efe-478e-b307-493364ff48b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ---> train loss 5.3849138663681835 ---> test loss 5.4383845165349625\n",
      "Epoch 2 ---> train loss 5.307152661587942 ---> test loss 5.3353072385260525\n",
      "Epoch 3 ---> train loss 5.089821841312972 ---> test loss 5.265667626319623\n"
     ]
    }
   ],
   "source": [
    "bert_model = bert_model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = model_train(bert_model, train_data_loader, optimizer, device, accumulation_steps=2)\n",
    "    test_loss = model_test(bert_model, test_data_loader, optimizer, device)\n",
    "    \n",
    "    print(f'Epoch {epoch+1} ---> train loss {train_loss} ---> test loss {test_loss}')\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b600c4c-f674-4bae-9442-39a601a176c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(model, tokenizer, context, question):\n",
    "    # Tokenize inputs\n",
    "    #inputs = tokenizer(context, question, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = tokenizer(\n",
    "            context,\n",
    "            question,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    input_ids = inputs['input_ids'].squeeze()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(**inputs)\n",
    "\n",
    "    # Get predicted answer\n",
    "    start_idx = torch.argmax(output.start_logits)\n",
    "    end_idx = torch.argmax(output.end_logits) + 1\n",
    "    \n",
    "    if end_idx < start_idx:\n",
    "        # Swap the indices if end_idx is less than start_idx\n",
    "        start_idx, end_idx = end_idx, start_idx\n",
    "    \n",
    "    answer = tokenizer.decode(input_ids[start_idx:end_idx])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47c8f2e4-069e-44d4-af2c-21824ab3e1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the lazy dog\n"
     ]
    }
   ],
   "source": [
    "context = \"The quick brown fox jumps over the lazy dog.\"\n",
    "question = \"What does the fox jump over?\"\n",
    "answer = predict_answer(bert_model, tokenizer, context, question)\n",
    "print(answer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2084fbdf-c605-437e-8725-50fa958880a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "washington\n"
     ]
    }
   ],
   "source": [
    "context = '''this main building and the library collection was entirely destroyed by a fire in april eighteen \n",
    "             seventy nine and the school closed immediately and students were sent home. the university founder \n",
    "             f r. soaring and the president at the time the rent. william corby immediately plan for the \n",
    "             rebuilding of the structures that have housed virtually the entire university. \n",
    "             construction was started on the seventeenth of may and by the incredible zeal of a administrator \n",
    "             and workers the building was completed before the fall semester of eighteen seventy nine. \n",
    "             the library collection was also rebuilt in spain housed in the new main building for years \n",
    "             afterwards. around the time of the fire the music hall was opened. eventually becoming known as \n",
    "             washington hall a hosted placing musical act put on by the school. by eighteen eighty as \n",
    "             science program was established at the university and a science hall today lafferty in student \n",
    "             center was built in eighteen eighty three. the hall housed multiple classrooms in science labs \n",
    "             needed for early research at the university.'''\n",
    "\n",
    "question = \"What was the music hall at Notre Dame called?\"\n",
    "\n",
    "answer = predict_answer(bert_model, tokenizer, context, question)\n",
    "\n",
    "print(answer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e305b8e5-6105-4445-b7f1-c04f83016a73",
   "metadata": {},
   "source": [
    "# Training & Evaluation - DistilBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21de52c7-f513-4074-a01c-4a836ea33244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ---> train loss 5.510384187986421 ---> test loss 5.601940179263173\n",
      "Epoch 2 ---> train loss 5.566739558144809 ---> test loss 5.58954277665804\n",
      "Epoch 3 ---> train loss 5.526833188829555 ---> test loss 5.491155148978012\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
    "\n",
    "# Initialize the DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased', max_length=512)\n",
    "distilbert_model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.AdamW(distilbert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "distilbert_model = distilbert_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = model_train(distilbert_model, train_data_loader, optimizer, device, accumulation_steps=2)\n",
    "    test_loss = model_test(distilbert_model, test_data_loader, optimizer, device)\n",
    "    print(f'Epoch {epoch+1} ---> train loss {train_loss} ---> test loss {test_loss}')\n",
    "\n",
    "    scheduler.step(test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23d36536-9d61-4e55-8a76-5cdc25b93ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the lazy dog\n"
     ]
    }
   ],
   "source": [
    "context = \"The quick brown fox jumps over the lazy dog.\"\n",
    "question = \"What does the fox jump over?\"\n",
    "answer = predict_answer(distilbert_model, tokenizer, context, question)\n",
    "print(answer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6a28b0a-307d-415d-be3e-1164a7dd9d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "washington\n"
     ]
    }
   ],
   "source": [
    "context = '''this main building and the library collection was entirely destroyed by a fire in april eighteen \n",
    "             seventy nine and the school closed immediately and students were sent home. the university founder \n",
    "             f r. soaring and the president at the time the rent. william corby immediately plan for the \n",
    "             rebuilding of the structures that have housed virtually the entire university. \n",
    "             construction was started on the seventeenth of may and by the incredible zeal of a administrator \n",
    "             and workers the building was completed before the fall semester of eighteen seventy nine. \n",
    "             the library collection was also rebuilt in spain housed in the new main building for years \n",
    "             afterwards. around the time of the fire the music hall was opened. eventually becoming known as \n",
    "             washington hall a hosted placing musical act put on by the school. by eighteen eighty as \n",
    "             science program was established at the university and a science hall today lafferty in student \n",
    "             center was built in eighteen eighty three. the hall housed multiple classrooms in science labs \n",
    "             needed for early research at the university.'''\n",
    "\n",
    "question = \"What was the music hall at Notre Dame called?\"\n",
    "\n",
    "answer = predict_answer(distilbert_model, tokenizer, context, question)\n",
    "\n",
    "print(answer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649bdfa-a291-4699-ac5a-629f728f7ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
